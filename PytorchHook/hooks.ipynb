{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.ones(5)\n",
    "a.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2 * a\n",
    "b.retain_grad() # to calculate the gradient of non-lead node\n",
    "c = b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
    }
   ],
   "source": [
    "c.backward()\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\ntensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
    }
   ],
   "source": [
    "# Redo the experiment but with a hook that multiplies b's grad by 2\n",
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2 * a\n",
    "b.retain_grad()\n",
    "\n",
    "b.register_hook(lambda x: print(x))\n",
    "\n",
    "b.mean().backward()\n",
    "\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([0.0800, 0.0800, 0.0800, 0.0800, 0.0800]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2 * a\n",
    "b.retain_grad()\n",
    "\n",
    "b.register_hook(lambda x: x * x)\n",
    "\n",
    "b.mean().backward()\n",
    "\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(module, grad_input, grad_output):\n",
    "    print(module)\n",
    "    print('---------------Input grad--------------------')\n",
    "\n",
    "    for grad in grad_input:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print('None found for Gradient')\n",
    "\n",
    "    print('---------------Output grad--------------------')\n",
    "    for grad in grad_output:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print('None found for Gradient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Linear(in_features=160, out_features=5, bias=True)\n---------------Input grad--------------------\ntorch.Size([5])\ntorch.Size([1, 160])\ntorch.Size([160, 5])\n---------------Output grad--------------------\ntorch.Size([1, 5])\nConv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n---------------Input grad--------------------\nNone found for Gradient\ntorch.Size([10, 3, 2, 2])\ntorch.Size([10])\n---------------Output grad--------------------\ntorch.Size([1, 10, 4, 4])\n"
    }
   ],
   "source": [
    "model = MyNet()\n",
    "model.conv.register_backward_hook(hook_fn)\n",
    "model.fc.register_backward_hook(hook_fn)\n",
    "\n",
    "sample_input = torch.randn((1, 3, 8, 8))\n",
    "sample_output = model(sample_input)\n",
    "\n",
    "(1 - sample_output.mean()).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        # No gradient shall be backprop conv less than 0\n",
    "        x.register_hook(lambda grad: torch.clamp(grad, min=0))\n",
    "        # Print whether there is any negative grad\n",
    "        x.register_hook(lambda grad: print(f'Gradients less than 0: {bool((grad < 0).any())gras.shap}'))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Gradients less than 0: torch.Size([1, 10, 4, 4])\nThe biases are: tensor([0., 0., 0., 0., 0.])\n"
    }
   ],
   "source": [
    "model = MyNet()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # if the param is from a linear and is a bias\n",
    "    if 'fc' in name and 'bias' in name:\n",
    "        param.register_hook(lambda grad: torch.zeros(grad.shape))\n",
    "\n",
    "pred = model(torch.randn((1, 3, 8, 8)))\n",
    "(1 - pred).mean().backward()\n",
    "\n",
    "print(f'The biases are: {model.fc.bias.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "conv\tConv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\nfc\tLinear(in_features=160, out_features=5, bias=True)\n"
    }
   ],
   "source": [
    "visualization = {}\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "    visualization[m] = o\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "for name, layer in model._modules.items():\n",
    "    # layer.register_forward_hook(hook_fn)\n",
    "    print(f'{name}\\t{layer}')\n",
    "    layer.register_forward_hook(hook_fn)\n",
    "    \n",
    "input = torch.randn((1, 3, 8, 8))\n",
    "\n",
    "pred = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride=2)\n",
    "        self.fc = nn.Linear(160, 5)\n",
    "        self.seq = nn.Sequential(nn.Linear(5, 3), nn.Linear(3, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\ttorch.Size([1, 10, 4, 4])\nLinear(in_features=160, out_features=5, bias=True)\ttorch.Size([1, 5])\nLinear(in_features=5, out_features=3, bias=True)\ttorch.Size([1, 3])\nLinear(in_features=3, out_features=2, bias=True)\ttorch.Size([1, 2])\n"
    }
   ],
   "source": [
    "visualization = {}\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "    visualization[m] = o\n",
    "\n",
    "def get_all_layers(model):\n",
    "    for name, layer in model._modules.items():\n",
    "        # If it is a sequential, don't register a hook on it but recursively register hook on all it's module children\n",
    "        if isinstance(layer, nn.Sequential):\n",
    "            get_all_layers(layer)\n",
    "        else:\n",
    "            # it's a non sequential. Register a hook\n",
    "            layer.register_forward_hook(hook_fn)\n",
    "\n",
    "model = MyNet()\n",
    "get_all_layers(model)\n",
    "pred = model(torch.randn((1, 3, 8, 8)))\n",
    "for m, o in visualization.items():\n",
    "    print(f'{m}\\t{o.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}